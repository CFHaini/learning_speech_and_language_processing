{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142ca93e",
   "metadata": {},
   "source": [
    "## NLP的发展历程 ：\n",
    "- 早期的规则基础方法\n",
    "- 统计方法\n",
    "- 机器学习和 深度学习\n",
    "\n",
    "## 在发展过程中出现了一些重要的深度学习模型：\n",
    "循环神经网络（RNN）\n",
    "长短时记忆网络（LSTM）\n",
    "注意力机制\n",
    "\n",
    "以及一些被用于词语（token）表示的方法，WordsVec模型开创了词向量表示的新时代。\n",
    "## 我对于Word2Vec的理解是 \n",
    "- 首先把训练语料拆解为词语单元，得到vocab_size\n",
    "- 把其中的每一个词元使用维度为n_embd的向量来表示，初始状态是Pytorch默认状态的\n",
    "- 给定上下文窗口d,对于语料中的每一个词元，在它的上下文中的其他词元被认为和这个选中的词元有关系\n",
    "而不在其中的词元，则认为当选中的词元出现时，其它的词元也不应该出现，即没有关系。对于词向量之间关系评价\n",
    "以之间的点积为评价标准，点积越大，就会认为词语之间会更接近.\n",
    "- 所以在构建训练数据时 有关系的positive_pair的个数为d，无关系的negative_pair的个数为k*d，k为给定系数\n",
    "- 之后就会将问题建模转化为二分类问题，来使得损失函数降低（BCELoss）\n",
    "- 在这个过程中我们并不太关心分类的情况，我们关心的时在损失函数降低的过程中embedding向量的参数更新\n",
    "- 在训练完成过，会得到有一定关联的词向量的表示\n",
    "  但是呢，Word2Vec的缺点是窗口的大小时受限的，其次是很难学习到词汇表之外的词语表示\n",
    "Word2Vec的pytorch实现在achievement_preject中的skip-gram.ipynb\n",
    "代码实现参考于\n",
    "https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/skipgram.ipynb\n",
    "\n",
    "## 有关的NLP任务：\n",
    "**中文分词** 即 是将连续的中⽂⽂本切分成有意义的词汇序列\n",
    "\n",
    "\n",
    "**子词切分** 是能够将词汇进一步分解为更小的单位，适用于在遇到罕见词或从未见过的新词时，能够通过已知的子词单位来理解或生成词汇 教程上列出了Byte Pair Encoding (BPE)、WordPiece、Unigram、SentencePiece，\n",
    "**但是我还没有去仔细地了解分词器的实现，我现在的词语分割都是通过简单的split()函数来分割，所以这方面我会重点观众**\n",
    "\n",
    "\n",
    "**词性标注** 给单词分配一个词性标签 **这方面我还没有了解过** ，教程上标注这个任务依赖于**机器学习模型**，\n",
    "但是我使用chatgpt了解到,也可以使用transformer的encoder部分来完成相关的词性标注和文本分类的任务，\n",
    "因为encoder模块时双向注意力，过去和未来都可以看到，适用于语句的理解。\n",
    "\n",
    "\n",
    "**实体识别** 仅仅通过教程了解了相关的概念，并未了解到实际的内容\n",
    "**关系抽取** 目标是从文本中识别实体之间的语义关系，对于构建知识图谱，提升机器理解语言的能力有重用作用\n",
    "**文本摘要** 这个部分我曾经了解过一点，可以使用decoder模块来自回归的实现，\n",
    "将需要总结的语料和高质量的摘要组合到一起，其中以一个特殊符号分隔，然后可以使用传统的decoder模块自回归的生成文本\n",
    "\n",
    "抽取式摘要  对于原文内容的抽取和拼接\n",
    "生成式摘要   需要理解文本的深层含义，以新的形式来表达相同的信息\n",
    "\n",
    "\n",
    "**机器翻译** Seq2Seq我没有了解过，而对于transformer模型，我还没有了解过涉及到多种向量空间的情况\n",
    "但是我曾经在学习transformer架构时\n",
    "![](../imgae/屏幕截图%202025-11-18%20160426.png)\n",
    "\n",
    "**encoder和decoder两个模块之间可以相互交流来实现不同向量空间之间的交流**，\n",
    "我感觉这样应该可以实现语句翻译，虽然当时学习的时候老师说这个情况适用于多模态\n",
    "\n",
    "\n",
    "\n",
    "**自动问答** 这个任务我完全没有了解过，根据教程可以了解到：\n",
    "其可以分为三类：检索式问答，知识库问答和社区问答。\n",
    "**检索式问答**通过搜索引擎等⽅式从⼤量⽂本中检索答案；\n",
    "**知识库问答**通过结构化的知识库来回答问题；\n",
    "**社区问答**则依赖于⽤户⽣成的问答数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0446ffc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b26cdb3b",
   "metadata": {},
   "source": [
    "## 文本表示的发展历程\n",
    "\n",
    "### 词向量 \n",
    "其将文本表示转换为高维空间中向量，在向量中每个维度代表向量中的不同特征项，其中的数值反应该文本对于这个特征的相关性或权重。\n",
    "\n",
    "产生的主要问题为：数据稀疏性和维度灾难问题\n",
    "\n",
    "### 语言模型\n",
    "N-gram我没有了解过，但是通过学习教程和chatgpt，我粗浅地理解为 统计+计数\n",
    "\n",
    "### WordVec\n",
    "我在上文已经了解并介绍了一部分\n",
    "\n",
    "### ELMo\n",
    " 用一个预训练的双向语言模型为每个词生成随上下文变化的向量表示，再把这些表示作为下游任务（NER、POS、QA 等）的输入特征。\n",
    " 教程上的内容介绍有点简略，我不太能明白，所以我使用了chatgpt进行讲解。\n",
    " ELMo = 预训练双向语言模型 + 取各层上下文表示 + 下游任务学习加权求和得到最终词向量。--chatgpt\n",
    "但是我还是不太明白，我没有详细地学习过机器学习中RNN和LSTM的部分。\n",
    "ELMo和transformer中的encoder的实现功能很相似，做上下文相关的编码\n",
    "\n",
    "我使用chatgpt,让他区分一下ELMo和encoder的区别，这样我就明白了\n",
    "\n",
    "```markdown\n",
    "### 不一样的地方\n",
    "1) 结构机制不同：循环 vs 自注意力\n",
    "\n",
    "ELMo（biLSTM）：信息通过 LSTM 一步步传递，位置 t 的表示来自“从左扫到 t”和“从右扫到 t”的隐藏状态。\n",
    "Encoder Transformer：用 self-attention，让每个位置直接“看见”全句所有位置并做加权汇聚（不依赖顺序递推）。\n",
    "结果：\n",
    "Transformer 更并行；biLSTM 更顺序、长距离信息传递更费劲。\n",
    "\n",
    "\n",
    "2) “双向”的方式不同：两个 LM vs 统一的双向编码\n",
    "\n",
    "ELMo：本质是 两个方向的语言模型（前向预测下一个词 + 后向预测上一个词），再把两边的隐藏状态合起来。左右信息是“分别建模，再融合”。\n",
    "BERT/Encoder：在同一个编码器里做双向 self-attention（训练常用 MLM），每层每个位置都能同时用左右文来形成表示。\n",
    "\n",
    "\n",
    "3) 训练目标与使用方式（经典做法）不同\n",
    "\n",
    "ELMo 经典用法更偏“特征抽取”：预训练好 biLM 后，下游任务通常冻结它，只学习“各层加权和”的权重 + 下游头。\n",
    "BERT 经典用法更偏“端到端微调”：下游任务往往会一起微调整个 Encoder（当然也能只当特征用）。\n",
    "\n",
    "\n",
    "4) 输入粒度不同（典型）\n",
    "\n",
    "ELMo：强强调字符级（char-CNN）得到词表示，对 OOV/拼写变化更友好。\n",
    "BERT：通常用子词分词（WordPiece/BPE），用子词组合解决 OOV。\n",
    "\n",
    "\n",
    "复制于chatgpt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9945543",
   "metadata": {},
   "source": [
    "学习的感受：第一章的内容结构很全面，但是里面的学习内容比较简略，但是我可以借助chatgpt来学习相关的内容，我很适应这种学习过程。\n",
    "给出你学习的方向，自己去找内容去学习，这样就明确了我的目标。（我最害怕的是不知道学习的步骤和目的，会有一种想学又不知道学什么的感觉）"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
